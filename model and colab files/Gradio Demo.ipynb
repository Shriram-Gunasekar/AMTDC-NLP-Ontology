{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56542,"status":"ok","timestamp":1688015619821,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"DRFbD8uRpWW8","outputId":"4cf444af-74a9-45d3-d427-75ea3165f4f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install transformers -q\n","!pip install bert-extractive-summarizer -q\n","!pip install sentence_transformers -q\n","!pip install PyPDF2 -q\n","!pip install gradio -q"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25350,"status":"ok","timestamp":1688015645162,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"o2eI_Zb6ATMh","outputId":"21b5b8a2-159f-41e3-feb4-541c6a37e4a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11203,"status":"ok","timestamp":1688015656360,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"h4tSdpPaBr6-"},"outputs":[],"source":["import pickle\n","import torch\n","from sentence_transformers import util\n","import json"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":3247,"status":"ok","timestamp":1688015659599,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"uxe7BcTDIT7s","outputId":"b6200072-5ef4-40f8-ab59-bb13df8a0fd7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' Multiclass Intent Classification for Chatbot Based \\non Machine Learning Algorithm  \\n \\nW. M. Amir Fazamin W. Hamzah \\nFaculty of Informatics and Computing  \\nUniversiti Sultan Zainal Abidin \\nTerengganu, Malaysia \\namirfazamin@unisza.edu.my \\nMokhairi Makhtar \\nFaculty of Informatics and Computing  \\nUniversiti Sultan Zainal Abidin \\nTerengganu, Malaysia \\nmokhairi@unisza.edu.my Mohd Kamir Yusof \\nFaculty of Informatics and Computing  \\nUniversiti Sultan Zainal Abidin \\nTerengganu, Malaysia \\nmohdkamir@unisza.edu.my \\nHasnah Nawang \\nFaculty of Informatics and Computing  \\nUniversiti Sultan Zainal Abidin \\nTerengganu, Malaysia \\nhasnah.nawang@kberang.mrsm.edu.myIsmahafezi Ismail \\nFaculty of Informatics and Computing  \\nUniversiti Sultan Zainal Abidin \\nTerengganu, Malaysia \\nismahafezi@unisza.edu.my \\nAzwa Abdul Aziz \\nFaculty of Informatics and Computing  \\nUniversiti Sultan Zainal Abidin \\nTerengganu, Malaysia \\nazwaaziz@gmail.com  \\nAbstract — In recent years, the use of Chatbots has grown \\nsignificantly in various industries, including support systems, \\neducation, health care, tourism, entertainment, and banking. \\nChatbot for education can provide instant feedback in \\ninteractions session with students. The responses from \\nChatbot can be based on machine learning algorithms or \\nmultiple heuristics techniques to select responses from the \\npredefined library. The generated response depends on the \\nuser’s intent of using the Chatbot. There are various classes \\nor categories in the user’s intent. However, the user’s intent \\nclass cannot simultaneously belong to multiple classes. \\nTherefore, this research proposed a multiclass intent \\nclassification for the Chatbot based on the machine learning \\nalgorithm. The findings of this research showed that Linear \\nSVC is the best machine learning algorithm model for \\nmulticlass intent classification. The results of the analysis \\nproved the accuracy of the prediction using Linear SVC.  \\nKeywords—chatbot, intent, classification, machine learning \\nI. INTRODUCTION  \\nToday\\'s advancement in digital technology presents a \\nwide use of Chatbots over time [1][2]. Chatbot in education \\nallows students to constantly interact with learning content. \\nUsing the Chatbot in education is an innovative way to \\novercome any barriers between technology and education. \\nIn addition, Chatbots can provide students with instant \\nfeedback during conversations [3]. The Chatbot can \\ngenerate initial responses based on machine learning \\nalgorithms or select responses from a predefined library \\nusing several heuristics techniques [4][5]. The generated \\nresponse depends on the user\\'s intent when using the \\nChatbot.  \\nThere are various classes or categories in user’s intent. \\nEach intent can be classified into one of the classes. \\nHowever, classes or categories for user’s intent cannot \\nbelong to more than one class simultaneously. The same \\nintent but categorised in many classes can cause inaccurate \\nresponses to Chatbot users. \\nTherefore, this research proposed a multiclass intent \\nclassification for Chatbot based on machine learning \\nalgorithm models to solve the problem of various classes or \\ncategories in the user\\'s intent. This solution explains the \\nsystematic method and evaluation metrics of multiclass \\nintent classification. The best model of machine learning \\nalgorithms will be used to predict the class of user intents. \\nMulticlass intent classification helps Chatbot to make better sense of user intents and find patterns. Using these user’s \\nintent patterns offers greater insights into making more \\naccurate responses. \\nII. RELATED WORK \\nA. Chatbot \\nChatbot is a computer programme that simulates human \\nlanguage using a text-based dialogue system [1]. Users can \\ncommunicate by text or voice input via a computer screen \\nwith text or audio/speech output. Intent refers to the goal of \\nthe Chatbot user when writing a question or comment. The \\nChatbot was initially developed using a simple keyword \\nmatching technique to find user input matches. After that, it \\nis developed using different pattern matching algorithms to \\nsimulate fiction or true personality [2][3]. \\nThere are two models of Chatbot architecture commonly \\nused for developing Chatbots; the generative model and the \\nretrieval-based model [6][7]. Generative models are \\ndifficult to construct and train. Typically, generative models \\nrequire millions of examples to train deep learning models \\nto achieve excellent conversation quality, and users cannot \\nbe certain of their responses. Meanwhile, retrieval-based \\nmodels are simpler to implement in Chatbot development, \\nwhich has the potential to produce more predictable \\nresponses [8][9]. \\nB. Classification \\nClassification is a predictive modelling problem in \\nwhich an example of input data is used to predict a class \\nlabel. From the point of view of modelling, classification \\nneeds a training dataset with many examples of inputs and \\noutputs to learn from. Classification is broadly distinguished \\ninto binary classification, multiclass classification and \\nmulti-label classification [10][11][12].  \\nBinary classification is when there are only two \\ncategories to classify data points. For example, detecting if \\na fruit is ripe (1) or not (0) or classifying whether COVID-\\n19 testing is positive (1) or negative (0).  \\nMulticlass classification is a type of classification task \\nin machine learning that has more than two outputs, or \\nclasses. A sample can only belong to one class when it \\ncomes to multiclass classification. For example, the \\nclassification of news content into news categories. The \\ncategories can be a crime, sports, business, entertainment, \\npolitics, and current issues.  2022 Seventh International Conference on Informatics and Computing (ICIC) | 979-8-3503-4571-1/22/$31.00 ©2022 IEEE | DOI: 10.1109/ICIC56845.2022.10006979\\nAuthorized licensed use limited to: ANNA UNIVERSITY. Downloaded on February 24,2023 at 06:01:11 UTC from IEEE Xplore.  Restrictions apply. Multi-label classification allows for the classification of \\ndatasets with more than one target variable. In multi-label \\nclassification, several labels become the outputs for a given \\nprediction. When making predictions, a given input may \\nbelong to more than one label. For example, when \\npredicting a given food taste category, it may belong to \\nsweet, salty, bitter, sour or all, simultaneously. In this \\nexample, the multi-labels can be assigned to a given food \\ntaste. \\nC. Machine Learning \\nMachine learning is related to the design of algorithms \\nthat allow computers to learn. Learning does not necessarily \\ninvolve awareness, but learning is a matter of finding \\nstatistical equations or other patterns in the data. Therefore, \\nan increased number of machine learning algorithms will \\nhardly resemble how humans can approach learning tasks. \\nHowever, learning algorithms can illustrate learning \\ndifficulties in different environments. Machine learning \\nalgorithms are structured into taxonomies, based on desired \\nalgorithm results. Common types of algorithms include \\nsupervised learning, unsupervised learning, semi-\\nsupervised learning, reinforcement learning and \\ntransduction [12][13]. \\nSupervised learning is a situation where input variables \\n(x) and output variables (y) learn the mapping function \\nthrough an algorithm from input to output y = f (x) to get \\nclose to the mapping function so that when new input data \\n(x) is available, predictions for output variables (y) can be \\nmade. Examples of supervised learning algorithms are \\nLogistic Regression, K-Nearest Neighbours, Decision \\nTrees, Random Forest, Naïve Bayes, and Support Vector \\nMachine [13][14][20].  \\nEach algorithm has its own advantages, such as the fact \\nthat Decision Trees do not require any assumptions about \\nthe linearity of the data and can therefore be utilised in \\nsituations where the parameters are non-linearly connected. \\nLogistic Regression can forecast the likelihood of a target \\nvariable. The target or dependent variable is binary. This \\nmeans that there will only be two classes. The categorical \\ndependent variable is best predicted using logistic \\nregression. The Random Forest is a set of Decision Trees. It \\nis a form of an ensemble method that aggregates the \\noutcomes of numerous predictors. The Random Forest also \\nemploys the bagging technique, which allows each tree to \\nbe trained on a random sample of the original dataset and \\ngets the majority vote from the trees. The Naïve Bayes \\nperforms well with categorical input variables, converges \\nfaster, and requires less training data than other \\ndiscriminative models like logistic regression. The \\nMultinomial Naïve Bayes has the best classification \\nperformance on the training data and is more effective at \\nclassifying the time ahead data accurately.  The Support \\nVector Machine determines the optimal way to classify data \\nbased on its position relative to a border between positive \\nand negative classes. This is the hyperplane, which \\nminimises the distance between data points of different \\nclassifications. Support Vector Machine, like Decision Tree \\nand Random Forest, can be used in both classification and \\nregression. The Linear SVC (linear support vector \\nclassifier) is for classification problems. \\n[13][14][15][16][20].  \\nHowever, all the selected algorithms must be tested to \\ndetermine the best algorithms for classification. The test must be done to ensure its performance before making the \\nfinal decision. The machine learning algorithm classifiers \\nsuch as Logistic Regression, Random Forest Classifiers, \\nMultinomial Naïve Bayes, and Linear Support Vector \\nClassifiers (Linear SVC) are usually used to perform \\nmulticlass classification. \\nD. Multiclass Classification Research \\nA lot of research on using multiclass classification has \\nbeen done before.  Some research includes Text \\nClassification, Image Classification, Malware \\nClassification, Medical Diagnosis, and Social Media \\nContent Analysis. \\n1) Text Classification: Moreo et al. have proposed \\nword-class embeddings (WCEs) to improve \\nmulticlass classification accuracy [21]. While \\nParmar et al. have done Multiclass Text \\nClassification and Analytics for Improving \\nCustomer Support Response through different \\nClassifiers [22]. \\n2) Image classification:  The research of multiclass skin \\ncancer image classification by convolutional neural \\nnetworks was done by Maron et al. for better \\nreflecting clinical differential diagnoses [23]. Vang \\net al. have proposed a deep learning framework for \\nthe multiclass breast cancer histology image \\nclassification problem [24]. \\n3) Malware classification:  Malware classification \\nresearch by Verma et al. has proposed binary texture \\nanalysis over greyscale images created directly from \\ntheir malware executables [25]. While Ghouti and \\nImam have done malware classification using \\ncompact image features and multiclass support \\nvector machines [26]. \\n4) Medical diagnosis:  David et al. have proposed a \\ndeep convolutionary Neural Network (DCNN) \\nbased architecture to diagnose and classify brain \\ntumours and assign grades to them [27]. Kuo et al., \\nin their research, have identified the appropriate \\ndiagnosis code for type 2 diabetes mellitus patients \\nby building a multi-class prediction model that is \\nboth parsimonious and possesses minimum features \\n[28]. \\n5) Social media content analysis:  Mustafa et al. have \\nproposed research on Multiclass Depression \\nDetection in Social Media Based on Sentiment \\nAnalysis [29]. While Bouazizi and Ohtsuki have \\ndone multiclass sentiment analysis on Twitter. That \\nresearch is about the feasibility of quantification and \\nproposed an approach to perform it on a data set \\nmade of tweets for 11 different sentiment classes \\n[30].  \\n \\nHowever, the research on multiclass intent \\nclassification is relatively less done. Therefore, this \\nresearch is worth doing to solve the problem of various \\nclasses or categories in the user\\'s intent of Chatbot. \\nAuthorized licensed use limited to: ANNA UNIVERSITY. Downloaded on February 24,2023 at 06:01:11 UTC from IEEE Xplore.  Restrictions apply. III. METHOD  \\nMulticlass intent classification is the process of \\nprecisely labelling an input in the form of a natural language \\nutterance from a predetermined set of intents. The machine \\nlearning model is trained to output a predicted classification \\nfor a given intent. There are various methods or approaches \\nto implement multiclass intent classification. The selection \\nof methods is usually based on the dataset\\'s type and the \\nnumber of data. Figure 1 shows the process for the \\nsystematic method and evaluation metrics of multiclass \\nintent classification based on the machine learning \\nalgorithm. The process includes data collection, data pre-\\nprocessing, dataset splitting, building a model, evaluation \\nmetrics and prediction [17]. \\n \\nFig. 1.  Multiclass Intent Classification Process \\nA. Data Collection \\nThe dataset used is a predefined library of Chatbot user’s \\nintent for the Web Application Development subject. The \\nintent contents are related to HTML, CSS, JavaScript, Java \\nServer Pages (JSP), Servlet and MySQL. Creating a \\ndictionary object encodes the intents classes as integer \\nvalues. \\nB. Data Pre-processing \\nData pre-processing is an important stage in developing \\na machine learning model, and its success is dependent on \\nhow effectively the data has been pre-processed. The data in \\nthis research is the user’s intent who use Chatbot, which is \\ntext-type data. Therefore, text pre-processing is used in this \\nresearch. Text pre-processing is the method of cleaning and \\npreparing text data in natural language processing (NLP). It \\nchanges text into a form that is easier to understand so that \\nalgorithms for machine learning can work better [18][19]. \\nThe text pre-processing steps that are involved are \\nStemming, Removing Stop Words, Lower Casing, \\nTokenization and Eliminating Unnecessary Characters. \\n1) Stemming: Stemming is the process of reducing a \\nword to its stem or root. It eliminates the word\\'s \\naffixes, leaving only the root.  \\n2) Removing Stop Words: The most prevalent words in \\nany language are stop words. However, they contribute little to the text\\'s clarity. Stop words \\nconsist of conjunctions, pronouns and articles. \\nEliminating stop words will allow the model to \\nconcentrate on training-relevant terms. \\n3) Lower Casing: The dataset is converted to \\nlowercase. \\n4) Tokenization: Separating sentences into smaller \\nword pieces is known as tokens. By examining the \\nword tokens, this method allows the model to \\ncomprehend sentences. \\n5) Eliminating unnecessary characters: The text \\ndataset may contain unnecessary characters that do \\nnot provide value to the model. Eliminating these \\ncharacters allows the model to concentrate on \\nessential information. \\nC. Dataset Splitting \\nSplitting the data into train and test sets. The data needs \\nto be pre-processed to be fed to the classification algorithm. \\nThe original data was split into features (X) and targets (y), \\nwhich were then split into train (75%) and test (25%) sets. \\nA reasonable rule of thumb is to test with 25% of dataset. \\nSo, the algorithms were trained on one dataset and tested on \\na completely different dataset. \\nD. Build a Model \\nTrain the machine learning algorithm classifiers models \\nsuch as Logistic Regression, Random Forest Classifiers, \\nMultinomial Naïve Bayes, and Linear SVC. Plot each \\nmodel\\'s performance to view the accuracy. Then, compare \\n\"mean accuracy\" and \"standard deviation\" to determine the \\nbest model. \\nE. Evaluation Metrics \\n1) Use the best model and make predictions: Train the \\nbest model to predict the test data. Find the most \\ncorrelated n-gram (unigrams and bigrams). The n-\\ngram model counts characters or word sequences to \\nprovide rich pattern finding in text. \\n2) Model evaluation and classification report: The \\nmodel performances are based on precision, recall \\nand F1-score. If all classes are balanced, accuracy is \\nan excellent starting point. When classes are \\nuneven, precision and recall become increasingly \\ncritical. Aim for more precision if false positive \\nforecasts are worse than false negatives. Aim for \\nincreased recall if erroneous negative predictions \\nare worse than false positives. The F1-score \\ncombines precision and recall. Classification reports \\nare made to obtain more insights into model \\nperformance. The confusion matrix conveys the \\nmodel’s right and wrong predictions on data. \\nF. Prediction \\nPredict unseen data. Predicting is done using the best \\nmodel. \\nIV. RESULT AND DISCUSSION  \\nThe dataset used contains 941 Chatbot intents (in the \\nform of text statements) related to HTML, CSS, JavaScript, \\nJava Server Pages (JSP), Servlet and MySQL. The intent \\nwas divided into three classes: ‘explanation’, ‘solution’ and 1. Data Collection\\n2. Data Pre-\\nprocessing\\n3. Dataset splitting\\n4. Build a Model5. Evaluation Metrics6. Prediction\\nAuthorized licensed use limited to: ANNA UNIVERSITY. Downloaded on February 24,2023 at 06:01:11 UTC from IEEE Xplore.  Restrictions apply. ‘code’ as shown in Table 1. \\'Class\\' refers to the categories \\nin the dataset, \\'User’s intent\\' refers to the user\\'s questions \\nwhen using the Chatbot, and \\'category id\\' refers to the user\\'s \\nintent classes in integer values. There was a class imbalance \\nthat may be a property of the problem domain. The \\n‘explanation’ class was seen to dominate the question in the \\ndataset. However, this did not affect the classification \\nprocess. \\nTABLE I.  INTENT CLASSES  \\nNo. Class User’s Intent Category \\nid \\n0 Explanation What is JSP? 0 \\n1 Explanation Tell me what is JSP? 0 \\n2 Explanation What is the meaning of JSP? 0 \\n3 Explanation Explain what is JSP. 0 \\n4 Explanation What is Java Server Page? 0 \\n… … … … \\n695 Solution How to resolve pass control \\nfrom one JSP page ... 1 \\n696 Solution How to mitigate pass control \\nfrom one JSP page... 1 \\n697 Solution How to stop pass control from \\none JSP page to ... 1 \\n698 Solution How to defend pass control \\nfrom one JSP page t... 1 \\n699 Solution How to get secured pass \\ncontrol from one JSP p... 1 \\n… … … … \\n936 Code Code example of JSP Implicit \\nObjects 2 \\n937 Code Code of JSP Implicit Objects 2 \\n938 Code Give me some sample code of \\nJSP request implicit... 2 \\n939 Code Code example of JSP request \\nimplicit object 2 \\n940 Code Code of JSP request implicit \\nobject 2 \\n \\nThe data pre-processing performed involved stemming, \\nremoving stop words, lower casing, tokenization and \\neliminating unnecessary characters. The dataset was divided \\ninto a train set (75%) and a test set (25%). Then, a model \\nperformance comparison was performed to determine the \\nbest machine learning algorithm model for multiclass intent \\nclassification. The box plot in Figure 2 displays the accuracy \\nof each machine learning algorithm. There were various box \\nplot shapes and positions. Logistic Regression, Random \\nForest Classifiers, Multinomial Naïve Bayes (Multinomial \\nNB) and Linear SVC have different centres. Linear SVC \\nwas entirely above other machine learning algorithms.  \\nFig. 2.  Box plot of machine learning algorithm models \\nTable 2 illustrates the data comparison of ‘mean \\naccuracy\\' and ‘standard deviation’ for each machine \\nlearning model. From the table, the accuracy of the Support \\nVector Machine classifier model, which is Linear SVC \\n(0.970292), outperformed all other machine learning \\nalgorithms. Meanwhile, Random Forest Classifier \\n(0.806625) was the machine learning algorithm with very \\nlow accuracy. Therefore, the Linear SVC was used for \\ntraining and making predictions. \\nTABLE II.  COMPARISON OF \"MEAN ACCURACY \" AND \"STANDARD \\nDEVIATION \" \\nModel Name Mean Accuracy Standard \\nDeviation \\nLinear SVC 0.970292 0.029611 \\nLogistic Regression 0.906496 0.016967 \\nMultinomial NB 0.915034 0.073687 \\nRandom Forest Classifier 0.806625 0.022289 \\n \\nFitting the Linear SVC model to the training data is \\nessentially the modelling process\\'s training. Generating text \\nusing the n-gram model counts characters or word \\nsequences of intents. The most correlated n-gram (unigrams \\nand bigrams) with each defined intent class is shown in \\nTable 3. Only the two most correlated n-grams are displayed \\nin the table.  \\nTABLE III.  THE MOST CORRELATED N -GRAM (UNIGRAMS AND \\nBIGRAMS  \\nClass Top correlated \\nunigrams Top correlated \\nbigrams \\nCode code  code jsp \\ntag code example \\nExplanation mean function components \\ncomponents jsp syntax \\nSolution defend requestdispatcher \\nservlet \\nmitigate define filters \\n \\nThe performance of the Linear SVC model is displayed \\nthrough the classification report and confusion matrix. \\nTable 4 shows the classification report. Precision, recall and \\nF-1 score were used to evaluate model performance. All the \\nAuthorized licensed use limited to: ANNA UNIVERSITY. Downloaded on February 24,2023 at 06:01:11 UTC from IEEE Xplore.  Restrictions apply. categories yielded better classification results. A clean \\ndataset contributed to this good result. According to the \\nconfusion matrix in Figure 3, the Linear SVC model has \\ndone a good job as it correctly predicted all the classes; \\n‘explanation’ (152), ‘solution’ (27) and ‘code’ (57). \\nTABLE IV.  CLASSIFICATION REPORT  \\n precision recall F1-score support \\n \\nExplanation 1.00 1.00 1.00 152 \\nSolution 1.00 1.00 1.00 27 \\nCode 1.00 1.00 1.00 57 \\n \\nAccuracy   1.00 236 \\nMacro avg 1.00 1.00 1.00 236 \\nWeighted avg 1.00 1.00 1.00 236 \\n \\n \\nFig. 3.  Confusion Matrix \\nAfter Linear SVC was trained, the model can be used to \\nmake predictions. Figure 4 displays the prediction of unseen \\ndata using the Linear SVC model. The class prediction that \\nhas been done is as follows. \\n• Predictions of class for the question \\'Give me \\ninformation about iteration in JSP?\\' is \\'explanation\\'. \\n• Predictions of class for the question \\'Give me some \\nsample code of add user in JSP?\\' is \\'code\\'. \\n• Predictions of class for the question \\'How to get \\nsecured against upload a file using Servlet?\\' is \\n\\'solution\\'. \\nAll the prediction results showed the correct prediction \\nof the class for each \\'question\\' of the intent class. The \\nnumber of classes affects the predicted results. This is \\nbecause a small class has a high probability of correct \\nprediction and is easier to predict.  \\nFig. 4.  Predict unseen data \\nV. CONCLUSION  \\nIn conclusion, this research has implemented the \\nsystematic method and evaluation metrics of multiclass \\nintent classification based on machine learning algorithm \\nmodels. This approach can be used to solve class issues that \\nare more than one class of intents simultaneously. The \\nresearch results revealed Linear SVC as a machine learning \\nalgorithm with high accuracy of 0.970292. Therefore, \\nLinear SVC was selected and used in doing multiclass intent \\nclassification. The performance of Linear SVC based on the \\nclassification report and confusion matrix showed high \\nvalues for \\'precision\\', \\'recall,\\' and \\'F1-score\\' represented by \\n1.00. Predictions made using Linear SVC showed that the \\npredicted \\'classes\\' were accurate based on the \\'questions\\' \\nasked. This prediction result is significant for understanding \\nuser intents and discovering learning patterns. This research \\nalso succeeded in finding the most correlated n-grams \\n(unigrams and bigrams) with each defined intent class. \\nACKNOWLEDGEMENT  \\nThe Ministry of Higher Education Malaysia (MOHE) \\nfunded this research through the Fundamental Research \\nGrant Scheme (FRGS), with the project reference code: \\nFRGS/1/2020/ICT06/UNISZA/02/3. Special thanks to the \\nCentre for Research Excellence and Incubation \\nManagement (CREIM) at Universiti Sultan Zainal Abidin \\n(UniSZA) for their assistance in carrying out this research. \\nREFERENCES  \\n[1] D. Zumstein and S. Hundertmark, “Chatbots: an interactive \\ntechnology for personalized communication and transaction,” Int. J . \\nWWW/Internet, 2018. \\n[2] A. Ho, J. Hancock, and A. S. Miner, “Psychological, relational, and \\nemotional effects of self-disclosure after conversations with a \\nchatbot,” J. Commun ., 2018, doi: 10.1093/joc/jqy026. \\n[3] B. R. Ranoliya, N. Raghuwanshi, and S. Singh, “Chatbot for \\nuniversity related FAQs,” in 2017 International Conference on \\nAdvances in Computing, Communications and Informatics, ICACCI  \\n2017, doi: 10.1109/ICACCI.2017.8126057. \\n[4] Y. Wu, W. Wu, C. Xing, C. Xu, Z. Li, and M. Zhou, “A sequential \\nmatching framework for multi-turn response selection in retrieval-\\nbased chatbots,” Comput. Linguist. , 2019, doi: \\n10.1162/coli_a_00345. \\n[5] Y. Wu, W. Wu, C. Xing, Z. Li, and M. Zhou, “Sequential matching \\nnetwork: A new architecture for multi-turn response selection in \\nretrieval-based chatbots,” in ACL 2017 - 55th Annual Meeting of the \\nAssociation for Computational Linguistics, Proceedings of the \\nConference  (Long Papers), 2017, doi: 10.18653/v1/P17-1046. \\nAuthorized licensed use limited to: ANNA UNIVERSITY. Downloaded on February 24,2023 at 06:01:11 UTC from IEEE Xplore.  Restrictions apply. [6] A. Bartl and G. Spanakis, “A retrieval-based dialogue system \\nutilizing utterance and context embeddings,” in Proceedings - 16th \\nIEEE International Conference on Machine Learning and \\nApplications, ICMLA  2017, doi: 10.1109/ICMLA.2017.00011. \\n[7] Y. Wu, Z. Li, W. Wu, and M. Zhou, “Response selection with topic \\nclues for retrieval-based chatbots,” Neurocomputing,  2018, doi: \\n10.1016/j.neucom.2018.07.073. \\n[8] W. M. A. F. Wan Hamzah, I. Ismail, M. K. Yusof, S. I. Mohd Saany \\nand A. Yacob, \"Using Learning Analytics to Explore Responses \\nfrom Student Conversations with Chatbot for Education\",  Int. J. \\nEng. Ped ., vol. 11, no. 6, pp. 70-84, Dec. 2021. \\n[9] E. Adamopoulou and L. Moussiades, “Chatbots: History, \\ntechnology, and applications,” Machine Learning with Applications , \\nvol. 2, p. 100006, 2020. \\n[10] de Carvalho, André CPLF, and Alex A. Freitas, “A tutorial on multi-\\nlabel classification techniques.” Foundations of computational \\nintelligence , volume 5, pp.177-195, 2009. \\n[11] Wang, Ran, Robert Ridley, Weiguang Qu, and Xinyu Dai, “A novel \\nreasoning mechanism for multi-label text classification.” \\nInformation Processing \u0026 Management,  58, no. 2, 102441, 2021. \\n[12] Sarker, Iqbal H, “Machine learning: Algorithms, real-world \\napplications and research directions.” SN Computer Science , vol. 2, \\nno. 3, pp. 1-21, 2021. \\n[13] Nurshahira Endut, W. M. Amir Fazamin W. Hamzah, Ismahafezi \\nIsmail, Mohd Kamir Yusof, Yousef Abu Baker and Hafiz Yusoff. \\n“A Systematic Literature Review on Multi-Label Classification \\nbased on Machine Learning Algorithms”, TEM Journal , 11(2), 658-\\n666, 2022. \\n[14] Punia, Sanjeev Kumar, Manoj Kumar, Thompson Stephan, Ganesh \\nGopal Deverajan, and Rizwan Patan, “Performance analysis of \\nmachine learning algorithms for big data classification: Ml and ai-\\nbased algorithms for big data analysis.” International Journal of E-\\nHealth and Medical Communications (IJEHMC) , vol. 12, no. 4, pp. \\n60-75, 2021. \\n[15] Ibrahim, Ibrahim, and Adnan Abdulazeez. “The role of machine \\nlearning algorithms for diagnosing diseases.”, Journal of Applied \\nScience and Technology Trends, vol. 2, no. 01, pp. 10-19, 2021. \\n[16] Elmogy, Ahmed M., Usman Tariq, Mohammed Ammar, and Atef \\nIbrahim. “Fake reviews detection using supervised machine \\nlearning.” International Journal of Advanced Computer Science and \\nApplications, vol. 12, no. 1, 2021. \\n[17] Bujang, Siti Dianah Abdul, Ali Selamat, Roliana Ibrahim, Ondrej \\nKrejcar, Enrique Herrera-Viedma, Hamido Fujita, and Nor Azura \\nMd Ghani. “Multiclass prediction model for student grade prediction \\nusing machine learning.”, IEEE Access, vol. 9, pp. 95608-95621, \\n2021. \\n[18] Anandarajan, Murugan, Chelsey Hill, and Thomas Nolan. “Text \\npreprocessing.” In Practical Text Analytics, Springer, Cham, pp. 45-\\n59, 2019. \\n[19] Keerthi Kumar, H. M., and B. S. Harish. “Classification of short text \\nusing various preprocessing techniques: An empirical evaluation.” \\nIn Recent findings in intelligent computing techniques, Springer, \\nSingapore, pp. 19-30, 2018. \\n[20] H. Nawang, M. Makhtar. and W.M.A.F. Wan Hamzah, \\n“Comparative analysis of classification algorithm evaluations to \\npredict secondary school students’ achievement in core and elective \\nsubjects”,  International Journal of Advanced Technology and \\nEngineering Exploration , 9(89), p.430, 2022.  \\n[21] Moreo, A., Esuli, A., and Sebastiani, F, Word-class embeddings for \\nmulticlass text classification. Data Mining and Knowledge \\nDiscovery , 35(3), 911-963, 2021  \\n[22] Parmar, P. S., Biju, P. K., Shankar, M., and Kadiresan, N, Multiclass \\ntext classification and analytics for improving customer support \\nresponse through different classifiers. In 2018 International \\nConference on Advances in Computing, Communications and \\nInformatics (ICACCI)   IEEE, pp. 538-542, 2018.  \\n[23] Maron, R. C., Weichenthal, M., Utikal, J. S., Hekler, A., Berking, \\nC., Hauschild, A., and Thiem, A., Systematic outperformance of 112 \\ndermatologists in multiclass skin cancer image classification by \\nconvolutional neural networks. European Journal of Cancer , 119, \\n57-65, 2019.  \\n[24] Vang, Y. S., Chen, Z., and Xie, X, Deep learning framework for \\nmulti-class breast cancer histology image classification. In International conference image analysis and recognition , \\nSpringer, Cham, pp. 914-922, 2018.  \\n[25] Verma, V., Muttoo, S. K., and Singh, V. B, Multiclass malware \\nclassification via first-and second-order texture \\nstatistics. Computers \u0026 Security , 97, 101895, 2020.  \\n[26] Ghouti, L., and Imam, M, Malware classification using compact \\nimage features and multiclass support vector machines. IET \\nInformation Security , 14(4), 419-429, 2020.  \\n[27] David, D. S., Saravanan, D., and Jayachandran, A, Deep \\nConvolutional Neural Network based Early Diagnosis of multi class \\nbrain tumour classification system. Solid State Technology , 63(6), \\n3599-3623, 2020.  \\n[28] Kuo, K. M., Talley, P., Kao, Y., \u0026 Huang, C. H, A multi-class \\nclassification model for supporting the diagnosis of type II diabetes \\nmellitus. PeerJ , 8, e9920, 2020.  \\n[29] Mustafa, R. U., Ashraf, N., Ahmed, F. S., Ferzund, J., Shahzad, B., \\nand Gelbukh, A, A multiclass depression detection in social media \\nbased on sentiment analysis. In 17th International Conference on \\nInformation Technology–New Generations (ITNG 2020) , Springer, \\nCham, pp. 659-662, 2020.  \\n[30] Bouazizi, M., and Ohtsuki, T., Multi-class sentiment analysis in \\nTwitter: What if classification is not the answer. IEEE Access , 6, \\n64486-64502, 2018.  \\n \\nAuthorized licensed use limited to: ANNA UNIVERSITY. Downloaded on February 24,2023 at 06:01:11 UTC from IEEE Xplore.  Restrictions apply. '"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from PyPDF2 import PdfReader\n","theirpdfdata = PdfReader('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/multiput.pdf')\n","theirpdfdata = ''.join([theirpdfdata.pages[i].extract_text() for i in range(len(theirpdfdata.pages))])\n","theirpdfdata"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":63726,"status":"ok","timestamp":1688015723317,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"-anFUvZtCEm4"},"outputs":[],"source":["QA = torch.load('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/QA')\n","Sim = torch.load('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/Sim')\n","Sum = torch.load('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/Sum')\n","geomvect = pickle.load(open('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/geomvectorizer', 'rb'))\n","gradevect = pickle.load(open('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/gradevectorizer', 'rb'))\n","profilevect = pickle.load(open('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/profileectorizer', 'rb'))\n","geomclass = pickle.load(open('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/geomclassifier', 'rb'))\n","gradeclass = pickle.load(open('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/gradeclassifier', 'rb'))\n","profileclass = pickle.load(open('/content/drive/MyDrive/Shriram Gunasekar/Demo Model Files/profileclassifier', 'rb'))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1688015723318,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"cPR71XswElQ5"},"outputs":[],"source":["def insertgeom(text):\n","    test_geom = geomvect.transform([text])\n","    return geomclass.predict(test_geom).item()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1688015723319,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"jPvM0D_0HDrC"},"outputs":[],"source":["def insertgrade(text):\n","    test_grade = gradevect.transform([text])\n","    return gradeclass.predict(test_grade).item()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1688015723320,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"RdgUJm-OHa6-"},"outputs":[],"source":["def insertprofile(text):\n","    test_profile = profilevect.transform([text])\n","    return profileclass.predict(test_profile).item()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1688015723321,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"0adVV5fJHW1p"},"outputs":[],"source":["def qanda(context,q):\n","    context = context\n","    question = q\n","    return QA({'context':context, 'question':question})['answer']"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1688015723321,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"rd_P-vmnB5p0"},"outputs":[],"source":["def sim(text1, text2):\n","    text1_representation = Sim.encode(text1)\n","    text2_representation = Sim.encode(text2)\n","    cosine_sim = util.pytorch_cos_sim(text1_representation,text2_representation)\n","    return round(cosine_sim.item(),3)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1688015723322,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"1oVudpK4C05M"},"outputs":[],"source":["def summ(text, numsent):\n","    return Sum(text, num_sentences=int(numsent))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":641,"status":"ok","timestamp":1688015723951,"user":{"displayName":"Shriram Gunasekar","userId":"00827184113820008148"},"user_tz":-330},"id":"LdxRsJbQaoFy"},"outputs":[],"source":["import gradio as gr"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":590},"id":"3KLuX3JXJpHu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://d701051363952d5a6c.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["\u003cdiv\u003e\u003ciframe src=\"https://d701051363952d5a6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"Use The Different Tabs\")\n","\n","    with gr.Tab(\" Q \u0026 A \"):\n","        qa_data = gr.Textbox(label=\"Enter Data\")\n","        qa_input = gr.Textbox(label=\"Enter Question\")\n","        qa_output = gr.Textbox(label=\"Your Answer\")\n","        qa_button = gr.Button('Get My Answer')\n","\n","    with gr.Tab(\" Summarizer \"):\n","        sum_data = gr.Textbox(label=\"Enter Data\")\n","        sum_input = gr.Textbox(label=\"Enter Number Of Sentences In Summary\")\n","        sum_output = gr.Textbox(label=\"Your Summary\")\n","        sum_button = gr.Button('Get My Summary')\n","\n","    with gr.Tab(\" Similarity \"):\n","        sim_input1 = gr.Textbox(label=\"Enter First Data\")\n","        sim_input2 = gr.Textbox(label=\"Enter Second Data\")\n","        sim_output = gr.Textbox(label=\"Your Similarity Score\")\n","        sim_button = gr.Button('Get Similarity Score')\n","\n","    with gr.Tab(\"Insert Grade\"):\n","        ig_input = gr.Textbox(label=\"Enter Insert Grade Description\")\n","        ig_output = gr.Textbox(label=\"Recommended Insert Grade\")\n","        ig_button = gr.Button('Get Insert Grade Recommendation')\n","\n","    with gr.Tab(\"Insert Geometry\"):\n","        ige_input = gr.Textbox(label=\"Enter Insert Geometry Description\")\n","        ige_output = gr.Textbox(label=\"Recommended Insert Geometry\")\n","        ige_button = gr.Button('Get Insert Geometry Recommendation')\n","\n","    with gr.Tab(\"Insert Profile\"):\n","        ip_input = gr.Textbox(label=\"Enter Insert Profile Description\")\n","        ip_output = gr.Textbox(label=\"Your Recommended Insert Geometry Description\")\n","        ip_button = gr.Button(label='Get Insert Profile Recommendation')\n","\n","\n","    qa_button.click(qanda, inputs=[qa_data, qa_input], outputs=qa_output)\n","    sim_button.click(sim, inputs=[sim_input1, sim_input2], outputs=sim_output)\n","    sum_button.click(summ, inputs=[sum_data, sum_input], outputs=sum_output)\n","    ig_button.click(insertgrade, inputs=[ig_input], outputs=ig_output)\n","    ige_button.click(insertgeom, inputs=[ige_input], outputs=ige_output)\n","    ip_button.click(insertprofile, inputs=[ip_input], outputs=ip_output)\n","\n","demo.launch(share=True,debug=True)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPNRgwn+sxo7NG5MvxfcSGB","mount_file_id":"1sHZbY_LZ05KPCLAFwE73gzRKZVKgK3zk","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}