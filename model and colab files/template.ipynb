{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWsgXCXi9uZE"
      },
      "source": [
        "##SpaCy Installation\n",
        "##Model en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwAdSFfu5CTL",
        "outputId": "3abd5c33-07ec-402a-9ade-b1b9e130c923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051301 sha256=36d89b75ec19de87664b5e87138178ff20cd7b8f61351506f5806cf566ff01fa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4wzplpgl/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcKtkUtIsBXD"
      },
      "source": [
        "##Initializing Model and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2qs7aRgsBXJ"
      },
      "outputs": [],
      "source": [
        "import spacy \n",
        "import en_core_web_md\n",
        "#Initialized Model\n",
        "nlp = spacy.load('en_core_web_md') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqldTeEBsBXK"
      },
      "source": [
        "##Regular vs Customized Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPh4FI57sBXL",
        "outputId": "31602eb5-8031-40de-f9f2-762b62637449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lemme', 'that']\n",
            "['lem', 'me', 'that']\n"
          ]
        }
      ],
      "source": [
        "from spacy.symbols import ORTH \n",
        "doc = nlp(\"lemme that\") \n",
        "print([w.text for w in doc])\n",
        "special_case = [{ORTH: \"lem\"}, {ORTH: \"me\"}] \n",
        "nlp.tokenizer.add_special_case(\"lemme\", special_case) \n",
        "print([w.text for w in nlp(\"lemme that\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpXFP5-fsBXM"
      },
      "source": [
        "##Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtYZO0KMsBXM",
        "outputId": "cdf9b13b-8bac-45b1-9b8e-8d43bbc3b0de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I flied to N.Y yesterday.\n",
            "It was around 5 pm.\n"
          ]
        }
      ],
      "source": [
        "text = \"I flied to N.Y yesterday. It was around 5 pm.\" \n",
        "doc = nlp(text)\n",
        "for sent in doc.sents: \n",
        "    print(sent.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R7vxpcfsBXM"
      },
      "source": [
        "##Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mY2WR9xsBXN",
        "outputId": "1d75aa27-a4b9-40e3-9304-364314a31c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I -PRON-\n",
            "went go\n",
            "there there\n",
            "for for\n",
            "working working\n",
            "and and\n",
            "worked work\n",
            "for for\n",
            "3 3\n",
            "years year\n",
            ". .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "doc = nlp(\"I went there for working and worked for 3 years.\") \n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhBbRivosBXN"
      },
      "source": [
        "##Customized Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1nMn3ksBXO",
        "outputId": "ef93b856-326d-499b-cf58-c3028865fd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I i\n",
            "am am\n",
            "flying flying\n",
            "to to\n",
            "Angeltown Los Angeles\n"
          ]
        }
      ],
      "source": [
        "from spacy.symbols import ORTH, NORM\n",
        "special_case = [{ORTH: 'Angeltown', NORM: 'Los Angeles'}] \n",
        "nlp.tokenizer.add_special_case('Angeltown', special_case)\n",
        "doc = nlp(u'I am flying to Angeltown') \n",
        "for token in doc:\n",
        "    print(token.text, token.norm_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8xpee2PsBXP"
      },
      "source": [
        "##JSON Serialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN2XjZoYsBXP",
        "outputId": "2f04bb0a-68be-40ca-e6b2-96e53426046c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Hi', 'ents': [], 'sents': [{'start': 0, 'end': 2}], 'tokens': [{'id': 0, 'start': 0, 'end': 2, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'ROOT', 'head': 0}]}\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Hi\") \n",
        "json_doc = doc.to_json()\n",
        "print(json_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb0HQR6BsBXP",
        "outputId": "d7d4c8a6-5a12-4b93-b3a4-89c42921cc01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Hi Sir, how are you? First meeting in two weeks', 'ents': [{'start': 21, 'end': 26, 'label': 'ORDINAL'}, {'start': 38, 'end': 47, 'label': 'DATE'}], 'sents': [{'start': 0, 'end': 20}, {'start': 21, 'end': 47}], 'tokens': [{'id': 0, 'start': 0, 'end': 2, 'pos': 'INTJ', 'tag': 'UH', 'dep': 'intj', 'head': 4}, {'id': 1, 'start': 3, 'end': 6, 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'npadvmod', 'head': 0}, {'id': 2, 'start': 6, 'end': 7, 'pos': 'PUNCT', 'tag': ',', 'dep': 'punct', 'head': 4}, {'id': 3, 'start': 8, 'end': 11, 'pos': 'ADV', 'tag': 'WRB', 'dep': 'advmod', 'head': 4}, {'id': 4, 'start': 12, 'end': 15, 'pos': 'AUX', 'tag': 'VBP', 'dep': 'ROOT', 'head': 4}, {'id': 5, 'start': 16, 'end': 19, 'pos': 'PRON', 'tag': 'PRP', 'dep': 'nsubj', 'head': 4}, {'id': 6, 'start': 19, 'end': 20, 'pos': 'PUNCT', 'tag': '.', 'dep': 'punct', 'head': 4}, {'id': 7, 'start': 21, 'end': 26, 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'head': 8}, {'id': 8, 'start': 27, 'end': 34, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ROOT', 'head': 8}, {'id': 9, 'start': 35, 'end': 37, 'pos': 'ADP', 'tag': 'IN', 'dep': 'prep', 'head': 8}, {'id': 10, 'start': 38, 'end': 41, 'pos': 'NUM', 'tag': 'CD', 'dep': 'nummod', 'head': 11}, {'id': 11, 'start': 42, 'end': 47, 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'pobj', 'head': 9}]}\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Hi Sir, how are you? First meeting in two weeks\") \n",
        "json_doc = doc.to_json() \n",
        "print(json_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTjvybq29RGi"
      },
      "source": [
        "#Dir Function \n",
        "#### Lists all the methods of the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfWEOjIr9K-H",
        "outputId": "3d6208a1-3edb-4b6e-f65a-c8a88f6b1175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " '_bulk_merge',\n",
              " '_py_tokens',\n",
              " '_realloc',\n",
              " '_vector',\n",
              " '_vector_norm',\n",
              " 'cats',\n",
              " 'char_span',\n",
              " 'count_by',\n",
              " 'doc',\n",
              " 'ents',\n",
              " 'extend_tensor',\n",
              " 'from_array',\n",
              " 'from_bytes',\n",
              " 'from_disk',\n",
              " 'get_extension',\n",
              " 'get_lca_matrix',\n",
              " 'has_extension',\n",
              " 'has_vector',\n",
              " 'is_nered',\n",
              " 'is_parsed',\n",
              " 'is_sentenced',\n",
              " 'is_tagged',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'mem',\n",
              " 'merge',\n",
              " 'noun_chunks',\n",
              " 'noun_chunks_iterator',\n",
              " 'print_tree',\n",
              " 'remove_extension',\n",
              " 'retokenize',\n",
              " 'sentiment',\n",
              " 'sents',\n",
              " 'set_extension',\n",
              " 'similarity',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'to_array',\n",
              " 'to_bytes',\n",
              " 'to_disk',\n",
              " 'to_json',\n",
              " 'to_utf8_array',\n",
              " 'user_data',\n",
              " 'user_hooks',\n",
              " 'user_span_hooks',\n",
              " 'user_token_hooks',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOUC87Yzqxmx"
      },
      "source": [
        "#Out of Vocabulary (OOV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "195CFxWGqxEY",
        "outputId": "a06a966e-d52e-40d6-bef8-aa3062589407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I False\n",
            "visited False\n",
            "pneumoultramicroscopic True\n",
            "at False\n",
            "AMTDC True\n",
            "internship False\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"I visited pneumoultramicroscopic at AMTDC internship\") \n",
        "for token in doc:\n",
        "    print(token, token.is_oov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir81XEY2rTjM"
      },
      "source": [
        "#General POS\n",
        "## Not much relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH4pgmnUstIj",
        "outputId": "1c7340ec-ddb8-4999-86fc-28ad87d9ffc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alicia PROPN NNP\n",
            "proper noun noun, proper singular\n",
            "and CCONJ CC\n",
            "coordinating conjunction conjunction, coordinating\n",
            "me PRON PRP\n",
            "pronoun pronoun, personal\n",
            "went VERB VBD\n",
            "verb verb, past tense\n",
            "to ADP IN\n",
            "adposition conjunction, subordinating or preposition\n",
            "the DET DT\n",
            "determiner determiner\n",
            "school NOUN NN\n",
            "noun noun, singular or mass\n",
            "by ADP IN\n",
            "adposition conjunction, subordinating or preposition\n",
            "bus NOUN NN\n",
            "noun noun, singular or mass\n",
            ". PUNCT .\n",
            "punctuation punctuation mark, sentence closer\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Alicia and me went to the school by bus.\") \n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)\n",
        "    print(spacy.explain(token.pos_), spacy.explain(token.tag_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIeaSwustBVn"
      },
      "source": [
        "##General Context Identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwPQnipbt9xh",
        "outputId": "2ebf9104-8b2c-473d-9c43-d0a5d29d9849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My DET PRP$\n",
            "determiner pronoun, possessive\n",
            "cat NOUN NN\n",
            "noun noun, singular or mass\n",
            "will VERB MD\n",
            "verb verb, modal auxiliary\n",
            "fish VERB VB\n",
            "verb verb, base form\n",
            "for ADP IN\n",
            "adposition conjunction, subordinating or preposition\n",
            "a DET DT\n",
            "determiner determiner\n",
            "fish NOUN NN\n",
            "noun noun, singular or mass\n",
            "tomorrow NOUN NN\n",
            "noun noun, singular or mass\n",
            "in ADP IN\n",
            "adposition conjunction, subordinating or preposition\n",
            "a DET DT\n",
            "determiner determiner\n",
            "fishy ADJ JJ\n",
            "adjective adjective\n",
            "way NOUN NN\n",
            "noun noun, singular or mass\n",
            ". PUNCT .\n",
            "punctuation punctuation mark, sentence closer\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"My cat will fish for a fish tomorrow in a fishy way.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)\n",
        "    print(spacy.explain(token.pos_), spacy.explain(token.tag_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7aioiT6uI5a"
      },
      "source": [
        "#Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ7VvbKAJDUN",
        "outputId": "dc06ff55-8137-4925-9b6b-65e7564df00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We PRP nsubj trying\n",
            "are VBP aux trying\n",
            "trying VBG ROOT trying\n",
            "to TO aux understand\n",
            "understand VB xcomp trying\n",
            "the DT det difference\n",
            "difference NN dobj understand\n",
            ". . punct trying\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"We are trying to understand the difference.\") \n",
        "for token in doc:\n",
        "    print(token.text, token.tag_, token.dep_, token.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WODqXxNUxyvA"
      },
      "source": [
        "###Matching With Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJJ3IpiJx1-m",
        "outputId": "0e9e0eff-a6d9-4756-af61-4e466c099c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 1 I\n",
            "3 4 grinding\n",
            "6 7 query\n",
            "11 12 types\n",
            "13 14 grinding\n"
          ]
        }
      ],
      "source": [
        "from spacy.matcher import Matcher \n",
        "doc = nlp(\"I have a grinding machine related query. What are the types of grinding machines?\") \n",
        "matcher = Matcher(nlp.vocab)\n",
        "p1 = [{\"TEXT\":\"I\"}]\n",
        "p2 = [{\"LOWER\":\"query\"}]\n",
        "p3 = [{\"LOWER\":\"grinding\"}]\n",
        "p4 = [{\"LOWER\":\"types\"}]\n",
        "matcher.add(\"p1\", [p1]) \n",
        "matcher.add(\"p2\", [p2])\n",
        "matcher.add(\"p3\", [p3])\n",
        "matcher.add(\"p4\", [p4])\n",
        "matches = matcher(doc) \n",
        "for mid, start, end in matches:\n",
        "    print(start, end, doc[start:end])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sURQ8CAzoc-z"
      },
      "source": [
        "### Phrase Matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioAtBGDrocLL",
        "outputId": "af8818a4-fb68-456c-b331-670bdb815908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 11 Angela Merkel\n",
            "16 18 Donald Trump\n",
            "22 24 Alexis Tsipras\n"
          ]
        }
      ],
      "source": [
        "from spacy.matcher import PhraseMatcher \n",
        "nlp = spacy.load(\"en_core_web_md\") \n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "terms = [\"Angela Merkel\", \"Donald Trump\", \"Alexis Tsipras\"] \n",
        "patterns = [nlp.make_doc(term) for term in terms] \n",
        "matcher.add(\"politiciansList\", None, *patterns) \n",
        "doc = nlp(\"3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.\") \n",
        "matches = matcher(doc) \n",
        "for mid, start, end in matches: \n",
        "    print(start, end, doc[start:end])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AMTDC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "45d06aa3a24b2dc9ce948a5c93cade23608a12caf84fe5b8bad374e81f5bd5e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
